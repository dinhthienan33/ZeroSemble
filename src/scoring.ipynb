{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'type'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 51\u001b[0m\n\u001b[0;32m     48\u001b[0m total_gt_triples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(triple_gt)\n\u001b[0;32m     50\u001b[0m mentions_gt_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mset\u001b[39m(m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmentions\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m mention_gt]\n\u001b[1;32m---> 51\u001b[0m metion_type_list \u001b[38;5;241m=\u001b[39m [m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m mention_gt]\n\u001b[0;32m     52\u001b[0m triple_gt_list \u001b[38;5;241m=\u001b[39m [(gt[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhead\u001b[39m\u001b[38;5;124m\"\u001b[39m], gt[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelation\u001b[39m\u001b[38;5;124m\"\u001b[39m], gt[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtail\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m gt \u001b[38;5;129;01min\u001b[39;00m triple_gt]\n\u001b[0;32m     54\u001b[0m GT[doc_id] \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmentions_GT\u001b[39m\u001b[38;5;124m\"\u001b[39m: mentions_gt_list, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelations_GT\u001b[39m\u001b[38;5;124m\"\u001b[39m: triple_gt_list, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmention_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: metion_type_list}\n",
      "\u001b[1;31mKeyError\u001b[0m: 'type'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "res_file = \"../attempts/qwen-2.5/results.json\"  # The path of the prediction file (results.json)\n",
    "ref_file = \"../attempts/qwen-2.5/results.json\"  # The path of the Ground True file (reference.json)\n",
    "output_dir = \"\"  # The path of the output directory\n",
    "scores_file = os.path.join(output_dir, \"scores.json\")  # The path of the output file (scores.json)\n",
    "\n",
    "# Counters\n",
    "EI_tp = 0\n",
    "EI_gold_len = 0\n",
    "EI_pred_len = 0\n",
    "EC_tp = 0\n",
    "EC_gold_len = 0\n",
    "EC_pred_len = 0\n",
    "RE_GEN_tp = 0\n",
    "RE_STRICT_tp = 0\n",
    "RE_gold_len = 0\n",
    "RE_pre_len = 0\n",
    "cnt = 0\n",
    "\n",
    "def safe_div(a, b):\n",
    "    return round(a / b * 100, 2) if b != 0 else 0.0\n",
    "\n",
    "def safe_div_(a, b):\n",
    "    return round(a / b, 2) if b != 0 else 0.0\n",
    "\n",
    "def compute_f1(cnt, tp, pred_num, gold_num):\n",
    "    return {\n",
    "        \"Total samples\": cnt,\n",
    "        \"P\": safe_div(tp, pred_num),\n",
    "        \"R\": safe_div(tp, gold_num),\n",
    "        \"F1\": safe_div_(2 * safe_div(tp, pred_num) * safe_div(tp, gold_num), safe_div(tp, pred_num) + safe_div(tp, gold_num))\n",
    "    }\n",
    "\n",
    "# Load Ground Truth\n",
    "with open(ref_file, \"r\", encoding=\"utf-8\") as gt_file:\n",
    "    Ground_True = json.load(gt_file)\n",
    "\n",
    "GT = {}\n",
    "total_gt_entities = 0\n",
    "total_gt_triples = 0\n",
    "\n",
    "for doc_id, sample in Ground_True.items():\n",
    "    mention_gt = sample[\"entities\"]\n",
    "    total_gt_entities += len(mention_gt)\n",
    "    triple_gt = sample[\"triples\"]\n",
    "    total_gt_triples += len(triple_gt)\n",
    "    \n",
    "    mentions_gt_list = [set(m[\"mentions\"]) for m in mention_gt]\n",
    "    metion_type_list = [m[\"type\"] for m in mention_gt]\n",
    "    triple_gt_list = [(gt[\"head\"], gt[\"relation\"], gt[\"tail\"]) for gt in triple_gt]\n",
    "    \n",
    "    GT[doc_id] = {\"mentions_GT\": mentions_gt_list, \"relations_GT\": triple_gt_list, \"mention_type\": metion_type_list}\n",
    "\n",
    "del Ground_True\n",
    "\n",
    "# Load Predictions\n",
    "with open(res_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    results = json.load(f)\n",
    "    cnt = len(results)\n",
    "\n",
    "total_pred_entities = 0\n",
    "total_pred_triples = 0\n",
    "\n",
    "for pre_id, sample in results.items():\n",
    "    mention_gt_list = GT[pre_id][\"mentions_GT\"]\n",
    "    relation_gt_list = GT[pre_id][\"relations_GT\"]\n",
    "    type_gt_list = GT[pre_id][\"mention_type\"]\n",
    "    \n",
    "    mention_pred = sample[\"entities\"]\n",
    "    total_pred_entities += len(mention_pred)\n",
    "    for i in range(len(mention_pred)):\n",
    "        mention_pred[i][\"mentions\"] = set(mention_pred[i][\"mentions\"])\n",
    "    \n",
    "    EI_gold_len += len(mention_gt_list)\n",
    "    EC_gold_len += len(mention_gt_list)\n",
    "    EI_pred_len += len(mention_pred)\n",
    "    EC_pred_len += len(mention_pred)\n",
    "\n",
    "    for i in range(len(mention_pred)):\n",
    "        for j in range(len(mention_gt_list)):\n",
    "            if mention_pred[i][\"mentions\"] in mention_gt_list:\n",
    "                EI_tp += 1\n",
    "                type_idx = mention_gt_list.index(mention_pred[i][\"mentions\"])\n",
    "                if mention_pred[i][\"type\"] == type_gt_list[type_idx]:\n",
    "                    EC_tp += 1\n",
    "                    break\n",
    "\n",
    "    triple_pred = sample[\"triples\"]\n",
    "    total_pred_triples += len(triple_pred)\n",
    "    RE_pre_len += len(triple_pred)\n",
    "    RE_gold_len += len(relation_gt_list)\n",
    "\n",
    "    for pred in triple_pred:\n",
    "        pred_triple = (pred[\"head\"], pred[\"relation\"], pred[\"tail\"])\n",
    "        if pred_triple in relation_gt_list:\n",
    "            RE_GEN_tp += 1\n",
    "            RE_STRICT_tp += 1\n",
    "\n",
    "# Print entity and triple counts\n",
    "print(f\"Ground Truth: {total_gt_entities} entities, {total_gt_triples} triples\",ref_file)\n",
    "print(f\"Predictions: {total_pred_entities} entities, {total_pred_triples} triples\",res_file)\n",
    "\n",
    "# Compute F1 Scores\n",
    "entity_identification_res = compute_f1(cnt, EI_tp, EI_pred_len, EI_gold_len)\n",
    "entity_classification_res = compute_f1(cnt, EC_tp, EC_pred_len, EC_gold_len)\n",
    "re_general_res = compute_f1(cnt, RE_GEN_tp, RE_pre_len, RE_gold_len)\n",
    "re_strict_res = compute_f1(cnt, RE_STRICT_tp, RE_pre_len, RE_gold_len)\n",
    "\n",
    "# Save results\n",
    "with open(scores_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\n",
    "        \"entity_ident\": entity_identification_res[\"F1\"],\n",
    "        \"entity_cla\": entity_classification_res[\"F1\"],\n",
    "        \"re_general\": re_general_res[\"F1\"],\n",
    "        \"re_strict\": re_strict_res[\"F1\"]\n",
    "    }, f, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Read both JSON files\n",
    "with open('ensemble1/results.json', 'r') as f1, open('rel.json', 'r') as f2:\n",
    "    data1 = json.load(f1)  # Source of entities (single dictionary)\n",
    "    data2 = json.load(f2)  # Target to receive entities (single dictionary)\n",
    "\n",
    "for key, value in data1.items():\n",
    "    if \"entities\" in value:\n",
    "        if data2.get(key):\n",
    "            if \"entities\" not in data2[key]:\n",
    "                data2[key][\"entities\"] = value[\"entities\"]\n",
    "\n",
    "with open(\"rel.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data2, f, ensure_ascii=False, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import ast  # Import for safer JSON parsing\n",
    "\n",
    "# Load dataset\n",
    "data_test = pd.read_csv(\"val_final_processed.csv\")\n",
    "results = {}\n",
    "# Iterate over the DataFrame\n",
    "for i, row in data_test.iterrows():\n",
    "    # Convert string to JSON (handle single & double quotes)\n",
    "    sample = ast.literal_eval(row[\"output\"].strip())  # More robust than json.loads\n",
    "    # save the sample to a json file, each sample in a new line, seperate by comma\n",
    "    results.update(sample)\n",
    "\n",
    "with open(\"val.json\", \"a\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The JSON files do not have the same structure.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def compare_json_structure(json1, json2):\n",
    "    if type(json1) != type(json2):\n",
    "        return False\n",
    "    \n",
    "    if isinstance(json1, dict):\n",
    "        if json1.keys() != json2.keys():\n",
    "            return False\n",
    "        for key in json1.keys():\n",
    "            if not compare_json_structure(json1[key], json2[key]):\n",
    "                return False\n",
    "    elif isinstance(json1, list):\n",
    "        if len(json1) != len(json2):\n",
    "            return False\n",
    "        for item1, item2 in zip(json1, json2):\n",
    "            if not compare_json_structure(item1, item2):\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "def check_json_files(file1, file2):\n",
    "    try:\n",
    "        with open(file1, 'r', encoding='utf-8') as f1, open(file2, 'r', encoding='utf-8') as f2:\n",
    "            json1 = json.load(f1)\n",
    "            json2 = json.load(f2)\n",
    "            return compare_json_structure(json1, json2)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading files: {e}\")\n",
    "        return False\n",
    "\n",
    "# Example usage\n",
    "file1 = 'val_result.json'  # Replace with your val_result2.json file path\n",
    "file2 = 'val.json'           # Replace with your val.json file path\n",
    "\n",
    "if check_json_files(file1, file2):\n",
    "    print(\"The JSON files have the same structure.\")\n",
    "else:\n",
    "    print(\"The JSON files do not have the same structure.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248\n",
      "The JSON file contains a dictionary with no string values.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def check_dict_in_json(file_path):\n",
    "    try:\n",
    "        # Load the JSON data from the file\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        print(len(data))\n",
    "        # Check if the loaded data is a dictionary\n",
    "        if isinstance(data, dict):\n",
    "            # Check if any value in the dictionary is a string\n",
    "            for key, value in data.items():\n",
    "                if isinstance(value, str):\n",
    "                    print(f\"The value for key '{key}' is a string: {value}\")\n",
    "                    return False\n",
    "            print(\"The JSON file contains a dictionary with no string values.\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"The JSON file does not contain a dictionary.\")\n",
    "            return False\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return False\n",
    "\n",
    "# Example usage\n",
    "file_path = 'qwen-2.5/results.json'  # Replace with your JSON file path\n",
    "check_dict_in_json(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processing complete. Saved to processed_results.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the data\n",
    "data = json.load(open('1/results.json', 'r', encoding='utf-8'))\n",
    "\n",
    "# Process the data\n",
    "for key, value in data.items():\n",
    "    if 'entities' in value:\n",
    "        entities = value['entities']\n",
    "        # Ensure entities is a list\n",
    "        if not isinstance(entities, list):\n",
    "            entities = [entities]\n",
    "        \n",
    "        processed_entities = []\n",
    "        for item in entities:\n",
    "            # If item is a string, convert it to a dictionary with a 'mentions' key\n",
    "            if isinstance(item, str):\n",
    "                processed_entities.append({'mentions': item})\n",
    "            # If item is a dictionary, ensure it has a 'mentions' key\n",
    "            elif isinstance(item, dict):\n",
    "                if 'mentions' not in item:\n",
    "                    item['mentions'] = ''  # or some default value\n",
    "                processed_entities.append(item)\n",
    "            # If item is a list, handle each element (you might need to adjust this based on your data structure)\n",
    "            elif isinstance(item, list):\n",
    "                processed_entities.extend([{'mentions': x} if isinstance(x, str) else x for x in item])\n",
    "        \n",
    "        # Update the entities with processed data\n",
    "        value['entities'] = processed_entities\n",
    "\n",
    "# Save the processed data to a new JSON file\n",
    "with open('qwen-2.5/results.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Data processing complete. Saved to processed_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mention_list(head_entity, mtlist):\n",
    "    \"\"\"\n",
    "    head_entity: Could be a string or a list of strings\n",
    "    mtlist: List of mentions to check against\n",
    "    \"\"\"\n",
    "    # Convert head_entity to a list if it isn't already\n",
    "    if not isinstance(head_entity, list):\n",
    "        head_entity = [head_entity]\n",
    "    \n",
    "    # Convert mtlist to a set for faster lookups if it's large\n",
    "    mtlist_set = set(mtlist)\n",
    "    \n",
    "    # Find all mentions that match any of the head_entity items\n",
    "    matches = []\n",
    "    for entity in head_entity:\n",
    "        if entity in mtlist_set:\n",
    "            matches.append(entity)\n",
    "    \n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing input files...\n",
      "  - Processing results.json\n",
      "  - Processing results.json\n",
      "  - Processing results.json\n",
      "\n",
      "Consolidating results...\n",
      "\n",
      "=== Document Combination Report ===\n",
      "Processed Files (3):\n",
      "  - results.json\n",
      "  - results.json\n",
      "  - results.json\n",
      "Unique Documents Combined: 248\n",
      "Total Unique Entities (across all docs): 11906\n",
      "Total Unique Triples (across all docs): 10247\n",
      "=================================\n",
      "\n",
      "✅ Combined document data successfully saved to: ensemble1/results.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "def read_json_file(file_path):\n",
    "    \"\"\"Reads a JSON file and returns its content, handling errors.\"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Warning: File not found - {file_path}. Skipping.\")\n",
    "        return None\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            if not isinstance(data, dict):\n",
    "                print(f\"Warning: File content is not a dictionary - {file_path}. Skipping.\")\n",
    "                return None\n",
    "            return data\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Warning: Invalid JSON format - {file_path}. Skipping.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Error reading file {file_path}: {str(e)}. Skipping.\")\n",
    "        return None\n",
    "\n",
    "def combine_documents(input_files):\n",
    "    \"\"\"Combines entities and triples for each document ID across multiple files, deduplicating within each document.\"\"\"\n",
    "    combined_docs = defaultdict(lambda: {\n",
    "        'title': None, \n",
    "        'entities': set(),  # Store hashable representations for deduplication\n",
    "        'triples': set()    # Store hashable representations for deduplication\n",
    "    })\n",
    "    valid_input_files = []\n",
    "    processed_doc_ids = set()\n",
    "\n",
    "    print(\"Processing input files...\")\n",
    "    for file_path in input_files:\n",
    "        data = read_json_file(file_path)\n",
    "        if data:\n",
    "            print(f\"  - Processing {os.path.basename(file_path)}\")\n",
    "            valid_input_files.append(os.path.basename(file_path))\n",
    "            for doc_id, doc in data.items():\n",
    "                processed_doc_ids.add(doc_id)\n",
    "                \n",
    "                # Store title (use first encountered)\n",
    "                if combined_docs[doc_id]['title'] is None:\n",
    "                    combined_docs[doc_id]['title'] = doc.get('title', f'Title not found in source files for {doc_id}')\n",
    "\n",
    "                # Add entities (deduplicated by set)\n",
    "                entities = doc.get('entities', [])\n",
    "                for entity in entities:\n",
    "                    try:\n",
    "                        # Use frozenset for mentions to handle order difference\n",
    "                        mentions_key = frozenset(entity.get('mentions', []))\n",
    "                        entity_type = entity.get('type', '')\n",
    "                        entity_key = (mentions_key, entity_type)\n",
    "                        combined_docs[doc_id]['entities'].add(entity_key)\n",
    "                    except TypeError:\n",
    "                        print(f\"    Warning: Skipping entity in doc {doc_id} due to unhashable mention: {entity.get('mentions')}\")\n",
    "                \n",
    "                # Add triples (deduplicated by set)\n",
    "                triples = doc.get('triples', [])\n",
    "                for triple in triples:\n",
    "                    try:\n",
    "                        head = triple.get('head', '')\n",
    "                        relation = triple.get('relation', '')\n",
    "                        tail = triple.get('tail', '')\n",
    "                        triple_key = (head, relation, tail)\n",
    "                        combined_docs[doc_id]['triples'].add(triple_key)\n",
    "                    except TypeError:\n",
    "                        print(f\"    Warning: Skipping triple in doc {doc_id} due to unhashable part: {triple}\")\n",
    "\n",
    "    # Convert sets back to lists of dictionaries for the final output\n",
    "    final_output_data = {}\n",
    "    total_entities_after = 0\n",
    "    total_triples_after = 0\n",
    "    print(\"\\nConsolidating results...\")\n",
    "    for doc_id, combined_info in combined_docs.items():\n",
    "        final_entities = []\n",
    "        for entity_key in combined_info['entities']:\n",
    "            mentions_set, entity_type = entity_key\n",
    "            final_entities.append({\n",
    "                'mentions': sorted(list(mentions_set)),  # Store consistently sorted\n",
    "                'type': entity_type\n",
    "            })\n",
    "        \n",
    "        final_triples = []\n",
    "        for triple_key in combined_info['triples']:\n",
    "            head, relation, tail = triple_key\n",
    "            final_triples.append({\n",
    "                'head': head,\n",
    "                'relation': relation,\n",
    "                'tail': tail\n",
    "            })\n",
    "            \n",
    "        final_output_data[doc_id] = {\n",
    "            'title': combined_info['title'],\n",
    "            'entities': sorted(final_entities, key=lambda x: (x['type'], x['mentions'][0] if x['mentions'] else '')),  # Sort for consistency\n",
    "            'triples': sorted(final_triples, key=lambda x: (x['head'], x['relation'], x['tail']))  # Sort for consistency\n",
    "        }\n",
    "        total_entities_after += len(final_entities)\n",
    "        total_triples_after += len(final_triples)\n",
    "\n",
    "    report = {\n",
    "        \"processed_files\": valid_input_files,\n",
    "        \"unique_documents_combined\": len(processed_doc_ids),\n",
    "        \"total_unique_entities_across_docs\": total_entities_after,\n",
    "        \"total_unique_triples_across_docs\": total_triples_after\n",
    "    }\n",
    "\n",
    "    return report, final_output_data\n",
    "\n",
    "def save_combined_data(data, output_file):\n",
    "    \"\"\"Saves the combined document data to a JSON file.\"\"\"\n",
    "    try:\n",
    "        output_dir = os.path.dirname(output_file)\n",
    "        if output_dir and not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "            print(f\"Created output directory: {output_dir}\")\n",
    "            \n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"\\n✅ Combined document data successfully saved to: {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Error saving combined data to {output_file}: {str(e)}\")\n",
    "\n",
    "def print_report(report):\n",
    "    \"\"\"Prints the summary report for document combination.\"\"\"\n",
    "    print(\"\\n=== Document Combination Report ===\")\n",
    "    print(f\"Processed Files ({len(report['processed_files'])}):\")\n",
    "    for filename in report['processed_files']:\n",
    "        print(f\"  - {filename}\")\n",
    "    print(f\"Unique Documents Combined: {report['unique_documents_combined']}\")\n",
    "    print(f\"Total Unique Entities (across all docs): {report['total_unique_entities_across_docs']}\")\n",
    "    print(f\"Total Unique Triples (across all docs): {report['total_unique_triples_across_docs']}\")\n",
    "    print(\"=================================\")\n",
    "\n",
    "# Example usage in Jupyter Notebook:\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace these with your actual file paths\n",
    "    input_files = [\n",
    "        \"deepseek/results.json\",\n",
    "        \"llama-3.3-70b-versatile/results.json\",\n",
    "        \"qwen-2.5/results.json\",\n",
    "        # Add more files as needed\n",
    "    ]\n",
    "    \n",
    "    output_file = \"ensemble1/results.json\"  # Change this to your desired output path\n",
    "    \n",
    "    # Process the files\n",
    "    report, combined_data = combine_documents(input_files)\n",
    "    \n",
    "    # Print the report\n",
    "    print_report(report)\n",
    "    \n",
    "    # Save the output\n",
    "    save_combined_data(combined_data, output_file)\n",
    "    \n",
    "    # The combined data is also available in the combined_data variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248\n"
     ]
    }
   ],
   "source": [
    "# read the json file\n",
    "with open(\"ensemble1/results.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# print the data\n",
    "print(len(data))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
